{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7cd6743f",
   "metadata": {},
   "source": [
    "# Introduction to Calling LLM APIs with Gemini and LangChain ü¶úüîó\n",
    "\n",
    "In this notebook you'll learn how to use LLM APIs through LangChain. We'll use Google's Gemini API as an example. By the end of this notebook, you will know how to make API calls using LangChain, and why we would do that."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48ee95ee",
   "metadata": {},
   "source": [
    "## ‚öôÔ∏è Setup\n",
    "\n",
    "üëâ Run the cell below to load the environment variables from the `.env` we created in the setup challenge:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1042cdd2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-21T10:31:17.130236Z",
     "iopub.status.busy": "2025-08-21T10:31:17.128867Z",
     "iopub.status.idle": "2025-08-21T10:31:17.180175Z",
     "shell.execute_reply": "2025-08-21T10:31:17.178572Z",
     "shell.execute_reply.started": "2025-08-21T10:31:17.130168Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv() # Load environment variables from .env file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1246c37e",
   "metadata": {},
   "source": [
    "üëâ Is the ouput of the cell \"`True`\"? Great! We have now set up a `GOOGLE_API_KEY` environment variable that will be used to authenticate with the Gemini API whenever we need it.\n",
    "\n",
    "If not, ask a TA for help."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e948d91",
   "metadata": {},
   "source": [
    "## Making a Simple API Call\n",
    "\n",
    "In this notebook, we will demonstrate how to:\n",
    "1. Make an API call using Google's own library.\n",
    "1. Do the same using LangChain."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afe02ed7",
   "metadata": {},
   "source": [
    "## Using Google Generative AI library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a30ef557",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-21T10:31:20.189627Z",
     "iopub.status.busy": "2025-08-21T10:31:20.189078Z",
     "iopub.status.idle": "2025-08-21T10:31:21.791928Z",
     "shell.execute_reply": "2025-08-21T10:31:21.790087Z",
     "shell.execute_reply.started": "2025-08-21T10:31:20.189577Z"
    }
   },
   "outputs": [],
   "source": [
    "from google import genai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9b461876",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-21T10:31:22.672313Z",
     "iopub.status.busy": "2025-08-21T10:31:22.671628Z",
     "iopub.status.idle": "2025-08-21T10:31:23.773011Z",
     "shell.execute_reply": "2025-08-21T10:31:23.771670Z",
     "shell.execute_reply.started": "2025-08-21T10:31:22.672252Z"
    }
   },
   "outputs": [],
   "source": [
    "client = genai.Client()\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=\"gemini-2.0-flash\",\n",
    "    contents=\"What is the capital of France?\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c051c73a",
   "metadata": {},
   "source": [
    "Have a look at the `response`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6585895d-b2d5-47db-9d96-09728c13f672",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-21T10:33:17.644473Z",
     "iopub.status.busy": "2025-08-21T10:33:17.643741Z",
     "iopub.status.idle": "2025-08-21T10:33:17.652624Z",
     "shell.execute_reply": "2025-08-21T10:33:17.650700Z",
     "shell.execute_reply.started": "2025-08-21T10:33:17.644405Z"
    }
   },
   "outputs": [],
   "source": [
    "# response.candidates[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "76fca992",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-21T10:33:18.821251Z",
     "iopub.status.busy": "2025-08-21T10:33:18.820606Z",
     "iopub.status.idle": "2025-08-21T10:33:18.834516Z",
     "shell.execute_reply": "2025-08-21T10:33:18.833143Z",
     "shell.execute_reply.started": "2025-08-21T10:33:18.821201Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The capital of France is **Paris**.\\n'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.candidates[0].content.parts[0].text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f91d5736",
   "metadata": {},
   "source": [
    "Do you see how you can get the actual answer out of it?\n",
    "\n",
    "Fortunately, we can also just use the `.text` attribute to immediately get the answer. Try it out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "75fd1681",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-21T10:33:26.796354Z",
     "iopub.status.busy": "2025-08-21T10:33:26.795589Z",
     "iopub.status.idle": "2025-08-21T10:33:26.810974Z",
     "shell.execute_reply": "2025-08-21T10:33:26.809667Z",
     "shell.execute_reply.started": "2025-08-21T10:33:26.796298Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The capital of France is **Paris**.\\n'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e11be5c",
   "metadata": {},
   "source": [
    "Gemini returns its answers in Markdown format. Let's use that!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c7b6cdb3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-21T10:33:38.910545Z",
     "iopub.status.busy": "2025-08-21T10:33:38.910123Z",
     "iopub.status.idle": "2025-08-21T10:33:38.917728Z",
     "shell.execute_reply": "2025-08-21T10:33:38.916415Z",
     "shell.execute_reply.started": "2025-08-21T10:33:38.910515Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "The capital of France is **Paris**.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Markdown\n",
    "Markdown(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5518ced2",
   "metadata": {},
   "source": [
    "You can also change the generation parameters. This is how you would do it using `google.genai`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4a60ed5e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-21T10:35:10.800143Z",
     "iopub.status.busy": "2025-08-21T10:35:10.796906Z",
     "iopub.status.idle": "2025-08-21T10:35:13.751133Z",
     "shell.execute_reply": "2025-08-21T10:35:13.749816Z",
     "shell.execute_reply.started": "2025-08-21T10:35:10.800071Z"
    }
   },
   "outputs": [],
   "source": [
    "from google import genai\n",
    "from google.genai import types # We need to import types for the config\n",
    "\n",
    "client = genai.Client()\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=\"gemini-2.0-flash\",\n",
    "    contents=\"Write a social media post about how much you're learning about transformers.\",\n",
    "    config=types.GenerateContentConfig(\n",
    "        max_output_tokens=300,\n",
    "        temperature=1\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "43e19b3e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-21T10:35:13.757222Z",
     "iopub.status.busy": "2025-08-21T10:35:13.754831Z",
     "iopub.status.idle": "2025-08-21T10:35:13.772341Z",
     "shell.execute_reply": "2025-08-21T10:35:13.766835Z",
     "shell.execute_reply.started": "2025-08-21T10:35:13.757166Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Okay, here's a social media post about learning about transformers, with a few variations to suit different platforms and tones:\n",
       "\n",
       "**Option 1: (General - Twitter/X)**\n",
       "\n",
       "> Diving deep into the world of Transformers! ü§Ø Honestly, my brain feels like it's learning a new language. Attention mechanisms, positional encoding... it's a lot, but SO fascinating. #Transformers #MachineLearning #NLP #AI #DeepLearning Learning something new every day! ü§ì\n",
       "\n",
       "**Option 2: (Slightly more technical - LinkedIn)**\n",
       "\n",
       "> Spending my time lately exploring the intricacies of Transformer networks.  The architecture is truly elegant, and the impact they've had on NLP and other fields is undeniable. I'm particularly interested in [mention a specific aspect you're studying, e.g., \"the application of transformers to time series forecasting\"].  Always striving to expand my knowledge in the AI/ML space! #AI #MachineLearning #DeepLearning #Transformers #NLP #ArtificialIntelligence #NeuralNetworks\n",
       "\n",
       "**Option 3: (More casual - Instagram/Facebook - Include a relevant image like a diagram of a transformer or you studying)**\n",
       "\n",
       "> Just me and my new best friend: the Transformer model! üòÇ Seriously, I'm trying to wrap my head around these things, and it's both challenging and incredibly rewarding. Anyone else find the self-attention mechanism mind-blowing?  Send help (and maybe some good resources"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Markdown(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d81e93c7",
   "metadata": {},
   "source": [
    "Cool. But imagine you want to try out another API, for example OpenAI's, or Anthropic's?\n",
    "\n",
    "You'd have to dive into their documentation, and rewrite all your code to use their API. Of course it will be similar, but not the same.\n",
    "\n",
    "Fortunately we have LangChain!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b5184b8",
   "metadata": {},
   "source": [
    "## Using LangChain ü¶úüîó"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4ca8092",
   "metadata": {},
   "source": [
    "Why would you use LangChain?\n",
    "\n",
    "1. **Model-Agnostic Code**\n",
    "\n",
    "   LangChain provides abstractions that let you swap between different LLM providers (Google, OpenAI, Anthropic, etc.) with minimal code changes. If you write code directly against the Google API, switching providers would require significant refactoring.\n",
    "\n",
    "2. **Unified Interface**\n",
    "\n",
    "   LangChain standardizes interactions across different LLM providers, offering consistent methods and response formats regardless of the underlying API.\n",
    "\n",
    "3. **Composability**\n",
    "\n",
    "   LangChain's chain and pipeline architecture makes it easier to build complex workflows combining prompts, memory, and retrieval without handling all the plumbing yourself.\n",
    "\n",
    "4. **Built-in Tools**\n",
    "\n",
    "   LangChain includes tools for output parsing, prompt templates, and other utilities that you'd otherwise need to implement yourself."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31766f84",
   "metadata": {},
   "source": [
    "Head to [LangChain's Python API Reference](https://python.langchain.com/api_reference/) and look at the list of integrations in the table of contents on the left. Can you find your favourite LLM provider?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97979490",
   "metadata": {},
   "source": [
    "In our code we don't want to use `chat_models.ChatGoogleGenerativeAI` because that is specifically made for Gemini. If we'd ever want to change the LLM, we'd have to change the way we instantiate the model. Fortunately LangChain offers a more generic way to instantiate a model.\n",
    "\n",
    "Let's use Gemini again, but now using LangChain's generic Chat Models. \n",
    "\n",
    "üëâ Head over to this page in the [LangChain's documentation](https://python.langchain.com/docs/tutorials/llm_chain/) and find how to instantiate a chat model using Gemini. \n",
    "\n",
    "Hints: \n",
    "1. Head immediately to the section \"Using Language Models\".\n",
    "1. You can select the model you want to use to immediately see the correct documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d677ae28",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-21T10:45:28.359539Z",
     "iopub.status.busy": "2025-08-21T10:45:28.358939Z",
     "iopub.status.idle": "2025-08-21T10:45:32.057976Z",
     "shell.execute_reply": "2025-08-21T10:45:32.056285Z",
     "shell.execute_reply.started": "2025-08-21T10:45:28.359482Z"
    },
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "! pip install -qU \"langchain[google-genai]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f1174d7b-5602-417d-b4ac-8820292d6d31",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-21T10:45:35.064182Z",
     "iopub.status.busy": "2025-08-21T10:45:35.063504Z",
     "iopub.status.idle": "2025-08-21T10:45:37.533042Z",
     "shell.execute_reply": "2025-08-21T10:45:37.530862Z",
     "shell.execute_reply.started": "2025-08-21T10:45:35.064125Z"
    }
   },
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "if not os.environ.get(\"GOOGLE_API_KEY\"):\n",
    "  os.environ[\"GOOGLE_API_KEY\"] = getpass.getpass(\"Enter API key for Google Gemini: \")\n",
    "\n",
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "model = init_chat_model(\"gemini-2.5-flash\", model_provider=\"google_genai\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eb80d54",
   "metadata": {},
   "source": [
    "The most basic utilisation of the model is to just use `.invoke()` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7ff0b7ff-ffd4-4937-a8b5-446f0f9803f4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-21T11:08:30.557775Z",
     "iopub.status.busy": "2025-08-21T11:08:30.557279Z",
     "iopub.status.idle": "2025-08-21T11:08:30.567282Z",
     "shell.execute_reply": "2025-08-21T11:08:30.565714Z",
     "shell.execute_reply.started": "2025-08-21T11:08:30.557730Z"
    }
   },
   "outputs": [],
   "source": [
    "# ?init_chat_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "194af798",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-21T11:03:54.574291Z",
     "iopub.status.busy": "2025-08-21T11:03:54.572389Z",
     "iopub.status.idle": "2025-08-21T11:03:55.104604Z",
     "shell.execute_reply": "2025-08-21T11:03:55.103223Z",
     "shell.execute_reply.started": "2025-08-21T11:03:54.574239Z"
    },
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(\"Translate the following from English into Italian\"),\n",
    "    HumanMessage(\"hi!\"),\n",
    "]\n",
    "\n",
    "response = model.invoke(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe8ef4d7",
   "metadata": {},
   "source": [
    "Let's have a look at the response. We use `pprint()` to pretty print the object's `__dict__` which contains all its attributes and methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7f2b989c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-21T11:03:56.734471Z",
     "iopub.status.busy": "2025-08-21T11:03:56.733948Z",
     "iopub.status.idle": "2025-08-21T11:03:56.745662Z",
     "shell.execute_reply": "2025-08-21T11:03:56.743909Z",
     "shell.execute_reply.started": "2025-08-21T11:03:56.734423Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'additional_kwargs': {},\n",
      " 'content': '**Ciao!**',\n",
      " 'example': False,\n",
      " 'id': 'run--a3dc1da8-6805-4318-958c-0909be1a21f6-0',\n",
      " 'invalid_tool_calls': [],\n",
      " 'name': None,\n",
      " 'response_metadata': {'finish_reason': 'STOP',\n",
      "                       'model_name': 'gemini-2.5-flash',\n",
      "                       'prompt_feedback': {'block_reason': 0,\n",
      "                                           'safety_ratings': []},\n",
      "                       'safety_ratings': []},\n",
      " 'tool_calls': [],\n",
      " 'type': 'ai',\n",
      " 'usage_metadata': {'input_token_details': {'cache_read': 0},\n",
      "                    'input_tokens': 10,\n",
      "                    'output_token_details': {'reasoning': 34},\n",
      "                    'output_tokens': 4,\n",
      "                    'total_tokens': 48}}\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "pprint(response.__dict__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1392f402-f0df-4be0-b5fe-a6f02279ad25",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-21T11:06:20.428052Z",
     "iopub.status.busy": "2025-08-21T11:06:20.427208Z",
     "iopub.status.idle": "2025-08-21T11:06:20.439537Z",
     "shell.execute_reply": "2025-08-21T11:06:20.438148Z",
     "shell.execute_reply.started": "2025-08-21T11:06:20.427984Z"
    }
   },
   "outputs": [],
   "source": [
    "# ?pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f4d05ba",
   "metadata": {},
   "source": [
    "Extract the answer, and display it. Remember it's Markdown, so you can make it appear pretty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9a8be751",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-21T11:07:04.768895Z",
     "iopub.status.busy": "2025-08-21T11:07:04.768416Z",
     "iopub.status.idle": "2025-08-21T11:07:04.778739Z",
     "shell.execute_reply": "2025-08-21T11:07:04.776946Z",
     "shell.execute_reply.started": "2025-08-21T11:07:04.768850Z"
    },
    "tags": [
     "challengify"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Ciao!**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# YOUR CODE HERE\n",
    "Markdown(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40a07b55",
   "metadata": {},
   "source": [
    "You can check the model's temperature accessing the `.temperature` attribute. Try it out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5bcd8fbd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-21T11:07:20.251280Z",
     "iopub.status.busy": "2025-08-21T11:07:20.250706Z",
     "iopub.status.idle": "2025-08-21T11:07:20.260834Z",
     "shell.execute_reply": "2025-08-21T11:07:20.259309Z",
     "shell.execute_reply.started": "2025-08-21T11:07:20.251233Z"
    },
    "tags": [
     "challengify"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# YOUR CODE HERE\n",
    "model.temperature"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcb5d8f3",
   "metadata": {},
   "source": [
    "We can also set the model's generation parameters before using it, by simply assigning a new value to the attributes.\n",
    "\n",
    "Try to code the equivalent of what we did using Google's library before to write a social media post:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5a8b1f64-1dd6-454a-a0ce-c662fbb33f90",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-21T11:12:33.312174Z",
     "iopub.status.busy": "2025-08-21T11:12:33.311595Z",
     "iopub.status.idle": "2025-08-21T11:12:33.325209Z",
     "shell.execute_reply": "2025-08-21T11:12:33.324018Z",
     "shell.execute_reply.started": "2025-08-21T11:12:33.312136Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m\n",
       "\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'LanguageModelInput'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mconfig\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'Optional[RunnableConfig]'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;34m*\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mcode_execution\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'Optional[bool]'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mstop\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'Optional[list[str]]'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'Any'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;34m'BaseMessage'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m\n",
       "Enable code execution. Supported on: gemini-1.5-pro, gemini-1.5-flash,\n",
       "gemini-2.0-flash, and gemini-2.0-pro. When enabled, the model can execute\n",
       "code to solve problems.\n",
       "\u001b[0;31mFile:\u001b[0m      ~/.pyenv/versions/3.12.9/envs/langchain-env/lib/python3.12/site-packages/langchain_google_genai/chat_models.py\n",
       "\u001b[0;31mType:\u001b[0m      method"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "?model.invoke"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "2d3a1675",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-21T11:23:04.303251Z",
     "iopub.status.busy": "2025-08-21T11:23:04.297840Z",
     "iopub.status.idle": "2025-08-21T11:23:08.295187Z",
     "shell.execute_reply": "2025-08-21T11:23:08.293240Z",
     "shell.execute_reply.started": "2025-08-21T11:23:04.303199Z"
    },
    "tags": [
     "steps"
    ]
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "config_0 = types.GenerateContentConfig(\n",
    "        max_output_tokens=200,\n",
    "        temperature=1)\n",
    "\n",
    "model_0 = init_chat_model(\"gemini-2.0-flash\", model_provider=\"google_genai\", configurable_fields=config_0)\n",
    "\n",
    "messages = [HumanMessage(\"Write a social media post about how much you're learning about transformers.\")]\n",
    "\n",
    "result_0 = model_0.invoke(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "09e4dc26-aa43-445a-b8d1-00747bacfc0a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-21T11:26:04.397517Z",
     "iopub.status.busy": "2025-08-21T11:26:04.397021Z",
     "iopub.status.idle": "2025-08-21T11:26:04.410652Z",
     "shell.execute_reply": "2025-08-21T11:26:04.409380Z",
     "shell.execute_reply.started": "2025-08-21T11:26:04.397475Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Okay, here's a social media post about learning about transformers:\n",
       "\n",
       "**Option 1 (Enthusiastic & General):**\n",
       "\n",
       "Just dove headfirst into the world of Transformers in NLP, and WOW! ü§Ø It's like unlocking a whole new level of understanding how machines process language. So much to learn about attention mechanisms, embeddings, and the magic behind models like BERT and GPT. Feeling a little overwhelmed but incredibly excited! #NLP #Transformers #MachineLearning #DeepLearning #AI #LearningJourney #AttentionIsAllYouNeed\n",
       "\n",
       "**Option 2 (A bit more specific & technical):**\n",
       "\n",
       "Spending my weekend deep-diving into the architecture of Transformers. Really fascinating how the self-attention mechanism allows the model to weigh the importance of different words in a sequence. Still grappling with the positional encoding, but I'm getting there! Anyone have any resources they found particularly helpful? #Transformers #NLP #SelfAttention #PositionalEncoding #DeepLearning #AI #MachineLearning #Study\n",
       "\n",
       "**Option 3 (Humorous & relatable):**\n",
       "\n",
       "My brain currently feels like it's trying to transform itself into a Transformer model. ü§ñ So much to learn! Just when I think I understand multi-head attention, I get hit with another layer of complexity. Wish me luck as I continue this journey! üòÖ #Transformers #NLP #MachineLearning #DeepLearning #AI #LearningIsHard #SendHelp #BrainsAreHard\n",
       "\n",
       "**Option 4 (Focus on a specific application):**\n",
       "\n",
       "Really excited about the potential of Transformers for [mention a specific application like text summarization, translation, or question answering]. Seeing how these models can generate coherent and contextually relevant text is mind-blowing. I'm experimenting with [mention a specific library or tool] and already seeing promising results! #Transformers #NLP #TextSummarization #MachineTranslation #QuestionAnswering #AI #DeepLearning #MachineLearning\n",
       "\n",
       "**Key things to consider when posting:**\n",
       "\n",
       "*   **Your audience:** Who are you trying to reach with your post? Adjust the technical level accordingly.\n",
       "*   **Your personality:** Make sure the post reflects your own voice and style.\n",
       "*   **Hashtags:** Use relevant hashtags to increase visibility.\n",
       "*   **Engagement:** Ask a question to encourage interaction.\n",
       "*   **Visuals:** Consider adding an image or GIF to make your post more engaging.  A diagram of a transformer, a funny AI meme, or even just a picture of you studying can help!"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Markdown(result_0.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c13e5ef",
   "metadata": {},
   "source": [
    "The advantage of this? This LangChain Chat Model can support many other APIs.\n",
    "\n",
    "The only things you'd need to change to switch to another model:\n",
    "1. Get an API key for the other model and define it in your code.\n",
    "1. When you instantiate the model switch out the model and provider."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43e2dda4",
   "metadata": {},
   "source": [
    "### Multiple messages\n",
    "\n",
    "Using the `.invoke()` function with just one message is a bit limiting.\n",
    "\n",
    "You can provide multiple messages, such as:\n",
    "- `SystemMessage`, or system messages: to tell the model how to behave\n",
    "- `HumanMessage`, or User messages: the input from the user\n",
    "- `AIMessage` or Assistant messages: a response from the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a20acae",
   "metadata": {},
   "source": [
    "Let's make a social media writer.\n",
    "\n",
    "We'll pass the model a system message to explain how to behave. Then in the user message, we can limit ourselves to just give it the subject to write about.\n",
    "\n",
    "Check the [LangChain documentation](https://python.langchain.com/docs/tutorials/llm_chain/) again to find out how to do this. Look for the section \"Using Language Models\" again.\n",
    "\n",
    "Need inspiration for the system message? Here's a basic instruction to get you started:\n",
    "\n",
    "```python\n",
    "\"\"\"You are a creative social media writer writing posts for a Gen AI student. \n",
    "Your posts always include a pun, and a call to action.\n",
    "Your posts are maximum 200 characters long.\n",
    "You always use emojis.\n",
    "\"\"\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "f8e258b1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-21T11:32:46.398633Z",
     "iopub.status.busy": "2025-08-21T11:32:46.397997Z",
     "iopub.status.idle": "2025-08-21T11:32:50.369168Z",
     "shell.execute_reply": "2025-08-21T11:32:50.368070Z",
     "shell.execute_reply.started": "2025-08-21T11:32:46.398587Z"
    },
    "tags": [
     "steps"
    ]
   },
   "outputs": [],
   "source": [
    "messages_1 = [SystemMessage(\"\"\"You are a creative social media writer writing posts for a Gen AI student. \n",
    "Your posts always include a pun, and a call to action.\n",
    "Your posts are minimum 200 characters long.\n",
    "You always use emojis.\n",
    "\"\"\"), HumanMessage(\"Write a post about my experinces as an Gen AI student.\")]\n",
    "\n",
    "\n",
    "response_2 = model_0.invoke(messages_1)\n",
    "# Import the necessary classes\n",
    "\n",
    "# YOUR CODE HERE\n",
    "\n",
    "# Create a list of messages\n",
    "\n",
    "# YOUR CODE HERE\n",
    "\n",
    "# Generate a response using the list of messages\n",
    "\n",
    "# YOUR CODE HERE\n",
    "\n",
    "# Display the response\n",
    "\n",
    "# YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "c7982d70-be14-430a-8d7a-7a1d3ef07692",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-21T11:32:50.371043Z",
     "iopub.status.busy": "2025-08-21T11:32:50.370639Z",
     "iopub.status.idle": "2025-08-21T11:32:50.379869Z",
     "shell.execute_reply": "2025-08-21T11:32:50.378507Z",
     "shell.execute_reply.started": "2025-08-21T11:32:50.371015Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Okay, here are a few options for your social media posts about your experiences as a Gen AI student, each with a pun, call to action, and emojis:\n",
       "\n",
       "**Option 1 (Focus on Learning):**\n",
       "\n",
       "> Diving deep into the world of Gen AI! ü§ñ It's a neural-ly exciting journey, packed with learning new things every day. From understanding algorithms to building creative applications, it's definitely not artificial! üòâ What are you waiting for? Learning AI is the future, and the future is now! üöÄ What are your favorite AI resources? Share them in the comments! üëá #GenAI #AIStudent #MachineLearning #ArtificialIntelligence #FutureTech #StudyAI\n",
       "\n",
       "**Option 2 (Focus on Challenges):**\n",
       "\n",
       "> Being a Gen AI student isn't always a smooth algorithm... sometimes you face tough bugs! üêõ But debugging and problem-solving is part of the fun! It's all about learning and growing! üå± Ready to take on the challenge? üí™ Let me know what challenges you face in the field! #GenAIStudent #AIChallenges #Debugging #MachineLearning #KeepLearning #TechLife #ArtificialIntelligence\n",
       "\n",
       "**Option 3 (Focus on Excitement):**\n",
       "\n",
       "> My brain is processing at lightning speed as a Gen AI student! ‚ö° The possibilities with AI are absolutely infinite, and I'm so excited to be a part of this revolution! ü§© It's an exponential adventure! üöÄ Ready to join the AI party? üéâ What AI projects are you working on? Tell me all about them in the comments! üëá #GenAI #AIRevolution #Innovation #FutureIsNow #MachineLearning #ArtificialIntelligence #Excited #TechStudent"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Markdown(response_2.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deb6b0f9",
   "metadata": {},
   "source": [
    "üèÅ Congratulations! You now master basic prompting using LangChain, using multiple messages."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
