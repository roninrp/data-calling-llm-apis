{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Calling LLM APIs with Gemini and LangChain ü¶úüîó\n",
    "\n",
    "In this notebook you'll learn how to use LLM APIs through LangChain. We'll use Google's Gemini API as an example. By the end of this notebook, you will know how to make API calls using LangChain, and why we would do that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚öôÔ∏è Setup\n",
    "\n",
    "üëâ Run the cell below to load the environment variables from the `.env` we created in the setup challenge:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv() # Load environment variables from .env file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üëâ Is the ouput of the cell \"`True`\"? Great! We have now set up a `GOOGLE_API_KEY` environment variable that will be used to authenticate with the Gemini API whenever we need it.\n",
    "\n",
    "If not, ask a TA for help."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making a Simple API Call\n",
    "\n",
    "In this notebook, we will demonstrate how to:\n",
    "1. Make an API call using Google's own library.\n",
    "1. Do the same using LangChain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Google Generative AI library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google import genai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = genai.Client()\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=\"gemini-2.0-flash\",\n",
    "    contents=\"What is the capital of France?\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Have a look at the `response`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response.candidates[0].content.parts[0].text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do you see how you can get the actual answer out of it?\n",
    "\n",
    "Fortunately, we can also just use the `.text` attribute to immediately get the answer. Try it out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gemini returns its answers in Markdown format. Let's use that!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown\n",
    "Markdown(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also change the generation parameters. This is how you would do it using `google.genai`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google import genai\n",
    "from google.genai import types # We need to import types for the config\n",
    "\n",
    "client = genai.Client()\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=\"gemini-2.0-flash\",\n",
    "    contents=\"Write a social media post about how much you're learning about transformers.\",\n",
    "    config=types.GenerateContentConfig(\n",
    "        max_output_tokens=200,\n",
    "        temperature=1.0\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Markdown(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cool. But imagine you want to try out another API, for example OpenAI's, or Anthropic's?\n",
    "\n",
    "You'd have to dive into their documentation, and rewrite all your code to use their API. Of course it will be similar, but not the same.\n",
    "\n",
    "Fortunately we have LangChain!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using LangChain ü¶úüîó"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why would you use LangChain?\n",
    "\n",
    "1. **Model-Agnostic Code**\n",
    "\n",
    "   LangChain provides abstractions that let you swap between different LLM providers (Google, OpenAI, Anthropic, etc.) with minimal code changes. If you write code directly against the Google API, switching providers would require significant refactoring.\n",
    "\n",
    "2. **Unified Interface**\n",
    "\n",
    "   LangChain standardizes interactions across different LLM providers, offering consistent methods and response formats regardless of the underlying API.\n",
    "\n",
    "3. **Composability**\n",
    "\n",
    "   LangChain's chain and pipeline architecture makes it easier to build complex workflows combining prompts, memory, and retrieval without handling all the plumbing yourself.\n",
    "\n",
    "4. **Built-in Tools**\n",
    "\n",
    "   LangChain includes tools for output parsing, prompt templates, and other utilities that you'd otherwise need to implement yourself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Head to [LangChain's Python API Reference](https://python.langchain.com/api_reference/) and look at the list of integrations in the table of contents on the left. Can you find your favourite LLM provider?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our code we don't want to use `chat_models.ChatGoogleGenerativeAI` because that is specifically made for Gemini. If we'd ever want to change the LLM, we'd have to change the way we instantiate the model. Fortunately LangChain offers a more generic way to instantiate a model.\n",
    "\n",
    "Let's use Gemini again, but now using LangChain's generic Chat Models. \n",
    "\n",
    "üëâ Head over to this page in the [LangChain's documentation](https://python.langchain.com/docs/tutorials/llm_chain/) and find how to instantiate a chat model using Gemini. \n",
    "\n",
    "Hints: \n",
    "1. Head immediately to the section \"Using Language Models\".\n",
    "1. You can select the model you want to use to immediately see the correct documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most basic utilisation of the model is to just use `.invoke()` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at the response. We use `pprint()` to pretty print the object's `__dict__` which contains all its attributes and methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "pprint(response.__dict__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract the answer, and display it. Remember it's Markdown, so you can make it appear pretty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can check the model's temperature accessing the `.temperature` attribute. Try it out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also set the model's generation parameters before using it, by simply assigning a new value to the attributes.\n",
    "\n",
    "Try to code the equivalent of what we did using Google's library before to write a social media post:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "steps"
    ]
   },
   "outputs": [],
   "source": [
    "# Set the maximum number of output tokens to 200\n",
    "\n",
    "# YOUR CODE HERE\n",
    "\n",
    "# Set the temperature to 1.0\n",
    "\n",
    "# YOUR CODE HERE\n",
    "\n",
    "# Generate a response with the new settings\n",
    "\n",
    "# YOUR CODE HERE\n",
    "\n",
    "# Display the response\n",
    "\n",
    "# YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The advantage of this? This LangChain Chat Model can support many other APIs.\n",
    "\n",
    "The only things you'd need to change to switch to another model:\n",
    "1. Get an API key for the other model and define it in your code.\n",
    "1. When you instantiate the model switch out the model and provider."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiple messages\n",
    "\n",
    "Using the `.invoke()` function with just one message is a bit limiting.\n",
    "\n",
    "You can provide multiple messages, such as:\n",
    "- `SystemMessage`, or system messages: to tell the model how to behave\n",
    "- `HumanMessage`, or User messages: the input from the user\n",
    "- `AIMessage` or Assistant messages: a response from the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make a social media writer.\n",
    "\n",
    "We'll pass the model a system message to explain how to behave. Then in the user message, we can limit ourselves to just give it the subject to write about.\n",
    "\n",
    "Check the [LangChain documentation](https://python.langchain.com/docs/tutorials/llm_chain/) again to find out how to do this. Look for the section \"Using Language Models\" again.\n",
    "\n",
    "Need inspiration for the system message? Here's a basic instruction to get you started:\n",
    "\n",
    "```python\n",
    "\"\"\"You are a creative social media writer writing posts for a Gen AI student. \n",
    "Your posts always include a pun, and a call to action.\n",
    "Your posts are maximum 200 characters long.\n",
    "You always use emojis.\n",
    "\"\"\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "steps"
    ]
   },
   "outputs": [],
   "source": [
    "# Import the necessary classes\n",
    "\n",
    "# YOUR CODE HERE\n",
    "\n",
    "# Create a list of messages\n",
    "\n",
    "# YOUR CODE HERE\n",
    "\n",
    "# Generate a response using the list of messages\n",
    "\n",
    "# YOUR CODE HERE\n",
    "\n",
    "# Display the response\n",
    "\n",
    "# YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üèÅ Congratulations! You now master basic prompting using LangChain, using multiple messages."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
